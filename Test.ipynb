{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #importing the necessary packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score\n",
    "#from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,roc_curve, roc_auc_score, precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from multiprocessing import Process# this is used for multithreading\n",
    "import multiprocessing\n",
    "import codecs# this is used for file operations \n",
    "import random as r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đọc dữ liệu\n",
    "df_train=pd.read_csv(\"NSL_KDD_Dataset\\KDDTrain+.txt\")\n",
    "df_test=pd.read_csv(\"NSL_KDD_Dataset\\KDDTest+.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "df_train.head() #Check 5 hàng đầu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "df_train.tail() #check 5 hàng cuối"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(\"Number of data points in train data\", df_train.shape)  \n",
    "print('-'*50)\n",
    "print(\"The attributes of data :\", df_train.columns.values) #check thuộc tính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(\"Number of data points in test data\", df_test.shape)\n",
    "print(df_test.columns.values)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "df_train.dtypes  #check kiểu dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "#Đổi tên sang 43 feature\n",
    "\n",
    "df_train = df_train.rename(columns={\"0\":\"Duration\",\"tcp\":\"protocol_type\",\"ftp_data\":\"service\",\"SF\":\"flag\",\"491\":\"src_bytes\",\n",
    "                                    \"0.1\":\"dest_bytes\",\"0.2\":\"Land\",\"0.3\":\"wrong_fragment\",\"0.4\":\"Urgent packets\",\"0.5\":\"hot\",\n",
    "                                    \"0.6\":\"num_failed_logins\",\"0.7\":\"logged_in\",\"0.8\":\"num_compromised\",\"0.9\":\"root_shell\",\n",
    "                                    \"0.10\":\"su_attempted\",\"0.11\":\"num_root\",\"0.12\":\"num_file_creations\",\"0.13\":\"num_shells\",\n",
    "                                    \"0.14\":\"num_access_files\",\"0.15\":\"num_outbound_cmds\",\"0.16\":\"is_host_login\",\"0.17\":\"is_guest_login\",\n",
    "                                    \"2\":\"count\",\"2.1\":\"srv_count\",\"0.00\":\"serror_rate\",\"0.00.1\":\"srv_serror_rate\",\"0.00.2\":\"rerror_rate\",\n",
    "                                    \"0.00.3\":\"srv_rerror_rate\",\"1.00\":\"same_srv_rate\",\"0.00.4\":\"diff_srv_rate\",\"0.00.5\":\"srv_diff_host_rate\",\n",
    "                                    \"150\":\"dst_host_count\",\"25\":\"dst_host_srv_count\",\"0.17.1\":\"dst_host_same_srv_rate\",\n",
    "                                    \"0.03\":\"dst_host_diff_srv_rate\",\"0.17.2\":\"dst_host_same_src_port_rate\",\n",
    "                                    \"0.00.6\":\"dst_host_srv_diff_host_rate\",\"0.00.7\":\"dst_host_serror_rate\",\n",
    "                                    \"0.00.8\":\"dst_host_srv_serror_rate\",\"0.05\":\"dst_host_rerror_rate\",\"0.00.9\":\"dst_host_srv_rerror_rate\",\n",
    "                                    \"normal\":\"attack_type\",\"20\":\"Score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "print(df_train['protocol_type'].value_counts())\n",
    "print(df_train['flag'].value_counts())\n",
    "\n",
    "df_train.head()  #check dataframe sau khi đổi tên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "\n",
    "#Tương tự với df_test \n",
    "\n",
    "df_test = df_test.rename(columns={\"0\":\"Duration\",\"tcp\":\"protocol_type\",\"private\":\"service\",\"REJ\":\"flag\",\"0.1\":\"src_bytes\",\n",
    "                                    \"0.2\":\"dest_bytes\",\"0.3\":\"Land\",\"0.4\":\"wrong_fragment\",\"0.5\":\"Urgent packets\",\"0.6\":\"hot\",\n",
    "                                    \"0.7\":\"num_failed_logins\",\"0.8\":\"logged_in\",\"0.9\":\"num_compromised\",\"0.10\":\"root_shell\",\n",
    "                                    \"0.11\":\"su_attempted\",\"0.12\":\"num_root\",\"0.13\":\"num_file_creations\",\"0.14\":\"num_shells\",\n",
    "                                    \"0.15\":\"num_access_files\",\"0.16\":\"num_outbound_cmds\",\"0.17\":\"is_host_login\",\"0.18\":\"is_guest_login\",\n",
    "                                    \"229\":\"count\",\"10\":\"srv_count\",\"0.00\":\"serror_rate\",\"0.00.1\":\"srv_serror_rate\",\"1.00\":\"rerror_rate\",\n",
    "                                    \"1.00.1\":\"srv_rerror_rate\",\"0.04\":\"same_srv_rate\",\"0.06\":\"diff_srv_rate\",\"0.00.2\":\"srv_diff_host_rate\",\n",
    "                                    \"255\":\"dst_host_count\",\"10.1\":\"dst_host_srv_count\",\"0.04.1\":\"dst_host_same_srv_rate\",\n",
    "                                    \"0.06.1\":\"dst_host_diff_srv_rate\",\"0.00.3\":\"dst_host_same_src_port_rate\",\n",
    "                                    \"0.00.4\":\"dst_host_srv_diff_host_rate\",\"0.00.5\":\"dst_host_serror_rate\",\n",
    "                                    \"0.00.6\":\"dst_host_srv_serror_rate\",\"1.00.2\":\"dst_host_rerror_rate\",\"1.00.3\":\"dst_host_srv_rerror_rate\",\n",
    "                                    \"neptune\":\"attack_type\",\"21\":\"Score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "# hàng trùng lặp\n",
    "duplicate_rows_df = df_train[df_train.duplicated()]\n",
    "print(\"number of duplicate rows: \", duplicate_rows_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Giá trị null\n",
    "print(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "\n",
    "label_encoder1 = preprocessing.LabelEncoder() \n",
    "df_train['protocol_type']= label_encoder1.fit_transform(df_train['protocol_type']) \n",
    "a=label_encoder1.classes_ \n",
    "label_encoder1.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "\n",
    "\n",
    "int_features=['tcp','private','REJ']\n",
    "int_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "\n",
    "for i in range(len(a)):\n",
    "        if a[i]==int_features[0]:\n",
    "            int_features[0]=i\n",
    "\n",
    "\n",
    "int_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exploratory Data Analysis\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "y_value_counts = df_train['attack_type'].value_counts()  #kiểm tra sự phân phối của các lớp khác nhau của từng nhãn\n",
    "y_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the bar plot of attack type variable to check the distribution of different class in the dataset-Train\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "y_value_counts.plot(kind=\"bar\", fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tương tự với test-data\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "y_test_value_counts = df_test['attack_type'].value_counts() \n",
    "y_test_value_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "y_test_value_counts.plot(kind=\"bar\", fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation: The above plot clearly shows that the attack type \"normal\" has the highest distribution in the data followed by \"neptune\" and then the other classes whose value count is very less compared to these two classes. The distribution is almost same for both test dataset and train dataset.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# counter = Counter(df_train['attack_type'])\n",
    "# a=dict(counter)\n",
    "# per=[]\n",
    "# for k,v in counter.items():\n",
    "# \tper.append(v / len(df_train['attack_type']) * 100) #calculating the percentage distribution of my class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Plotting the pie chart of attack type with the percentage distribution of each attack type \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# patches, texts = plt.pie(per, startangle=90, radius=2)  #https://stackoverflow.com/questions/23577505/how-to-avoid-overlapping-of-labels-autopct-in-a-matplotlib-pie-chart\n",
    "# labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(a.keys(), per)]\n",
    "# patches, labels, dummy =  zip(*sorted(zip(patches, labels, per),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "\n",
    "# plt.legend(patches,labels , loc='center left', bbox_to_anchor=(-0.1, 1.), fontsize=8)\n",
    "\n",
    "# plt.savefig('piechart.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above plot gives an idea of the percentage value of each class. The normal class covers almost 53% of the data followed by neptune class which covers 32% and then the rest of the classes each covering less than 3% of the entire dataset. From the above plot we can conclude that our dataset is an imbalanced dataset with huge difference in the distribution of different class labels\n",
    "\n",
    "# Lets have a look at the distribution of each feature of the dataframe.\n",
    "# \n",
    "# ---\n",
    "# \n",
    "# ---\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# df_train.hist(figsize=(35,35)) \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Now lets view the correlation between features and target variable.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#Ma trận tương quan\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import phik\n",
    "from phik import resources, report\n",
    "corr_matrix=df_train.phik_matrix()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(corr_matrix[\"attack_type\"].sort_values(ascending=False)[1:])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "corr = corr_matrix[\"attack_type\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_sep={'normal':\"Normal\",'neptune':\"DOS\",\n",
    "            'satan':\"Probe\",'ipsweep':\"Probe\",'named':\"R2L\",\n",
    "            'ps':\"U2R\",'sendmail':\"R2L\",'xterm':\"U2R\",'xlock':\"R2L\",\n",
    "            'xsnoop':\"R2L\",'udpstorm':\"DOS\",'sqlattack':\"U2R\",'worm':\"DOS\",'portsweep':\"Probe\",\n",
    "            'smurf':\"DOS\",'nmap':\"Probe\",'back':\"DOS\",'mscan':\"Probe\",'apache2':\"DOS\",'processtable':\"DOS\",\n",
    "            'snmpguess':\"R2L\",'saint':\"Probe\",'mailbomb':\"DOS\",'snmpgetattack':\"R2L\",'httptunnel':\"R2L\",'teardrop':\"DOS\",\n",
    "            'warezclient':\"R2L\",'pod':\"DOS\",'guess_passwd':\"R2L\",'buffer_overflow':\"U2R\",'warezmaster':\"R2L\",'land':\"DOS\",'imap':\"R2L\",\n",
    "            'rootkit':\"U2R\",'loadmodule':\"U2R\",'ftp_write':\"R2L\",'multihop':\"R2L\",'phf':\"R2L\",'perl':\"U2R\",'spy':\"R2L\"}\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "df_train.replace({'attack_type':attack_sep},inplace=True)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "df_test.replace({'attack_type':attack_sep},inplace=True)\n",
    "\n",
    "\n",
    "# # Lets train a base model on the entire dataset and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "\n",
    "from sklearn import preprocessing \n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "df_train['protocol_type']= label_encoder.fit_transform(df_train['protocol_type']) \n",
    "df_test['protocol_type']= label_encoder.fit_transform(df_test['protocol_type']) \n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "df_train['service']= label_encoder.fit_transform(df_train['service']) \n",
    "df_test['service']= label_encoder.transform(df_test['service']) \n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "df_train['flag']= label_encoder.fit_transform(df_train['flag']) \n",
    "df_test['flag']= label_encoder.transform(df_test['flag']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "#tách feature và labels\n",
    "\n",
    "y=df_train['attack_type']   #labels\n",
    "X=df_train.drop(['attack_type'],axis=1) #feature\n",
    "\n",
    "y_test=df_test['attack_type']\n",
    "X_test=df_test.drop(['attack_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "\n",
    "\n",
    "sc = StandardScaler()  #Chuẩn hóa dữ liệu\n",
    "X_train = sc.fit_transform(X)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets build a base model on our dataset\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "def falseposrate(conf_matrix,y_test,pred):\n",
    "  FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix) \n",
    "  FN = conf_matrix.sum(axis=1) - np.diag(conf_matrix)\n",
    "  TP = np.diag(conf_matrix)\n",
    "  TN = conf_matrix.sum() - (FP + FN + TP)\n",
    "  FP = FP.astype(float)\n",
    "  FN = FN.astype(float)\n",
    "  TP = TP.astype(float)\n",
    "  TN = TN.astype(float)\n",
    "  FPR = FP/(FP+TN)\n",
    "  recall = recall_score(y_test, pred,average='micro')\n",
    "  precision = precision_score(y_test, pred,average='micro')\n",
    "  return FPR,recall,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "#sử dụng Support Vector Machine\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "# clf= svm.SVC(kernel='linear',probability=True)\n",
    "# clf.fit(X_train,y) #learning\n",
    "# pred = clf.predict(X_test) #kết quả sau khi test\n",
    "# recall = recall_score(y_test, pred,average='micro')\n",
    "# precision = precision_score(y_test, pred,average='micro')\n",
    "# score = metrics.accuracy_score(y_test, pred)\n",
    "# f1score= f1_score(y_test, pred, average='micro')\n",
    "# print(\"Accuracy :\",score)\n",
    "# print('=' * 50)\n",
    "# print(\"F1 score :\",f1score)\n",
    "\n",
    "# cnf_matrix = confusion_matrix(y_test, pred) #confusion matrix của nhẫn tấn công (test data) và kết quả sau khi test\n",
    "# #sns.heatmap(cnf_matrix)\n",
    "# fig, ax = plt.subplots(figsize=(15, 8))\n",
    "# disp = plot_confusion_matrix(clf, X_test, y_test,ax=ax,cmap=plt.cm.Blues)\n",
    "# plt.show()\n",
    "\n",
    "# print('_' * 50)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "# FPR= falseposrate(cnf_matrix, y_test, pred)\n",
    "# print('=' * 50)\n",
    "# print(\"|False positive Rate :|\")\n",
    "# print('=' * 50)\n",
    "# print(FPR)\n",
    "# print('=' * 50)\n",
    "# print(\"|Precision:|\")\n",
    "# print('=' * 50)\n",
    "# print(precision)\n",
    "# print('=' * 50)\n",
    "# print(\"|recall:|\")\n",
    "# print('=' * 50)\n",
    "# print( recall)\n",
    "# print('=' * 50)\n",
    "# print(\"|Classification report|\")\n",
    "# print('=' * 50)\n",
    "# print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "#sử dụng DecisionTreeClassifier\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "clf= DecisionTreeClassifier()\n",
    "clf.fit(X_train,y) #learning\n",
    "pred = clf.predict(X_test) #kết quả sau khi test\n",
    "recall = recall_score(y_test, pred,average='micro')\n",
    "precision = precision_score(y_test, pred,average='micro')\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "f1score= f1_score(y_test, pred, average='micro')\n",
    "print(\"Accuracy :\",score)\n",
    "print('=' * 50)\n",
    "print(\"F1 score :\",f1score)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, pred) #confusion matrix của nhẫn tấn công (test data) và kết quả sau khi test\n",
    "#sns.heatmap(cnf_matrix)\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "disp = plot_confusion_matrix(clf, X_test, y_test,ax=ax,cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "print('_' * 50)\n",
    "print(cnf_matrix)\n",
    "\n",
    "FPR= falseposrate(cnf_matrix, y_test, pred)\n",
    "print('=' * 50)\n",
    "print(\"|False positive Rate :|\")\n",
    "print('=' * 50)\n",
    "print(FPR)\n",
    "print('=' * 50)\n",
    "print(\"|Precision:|\")\n",
    "print('=' * 50)\n",
    "print(precision)\n",
    "print('=' * 50)\n",
    "print(\"|recall:|\")\n",
    "print('=' * 50)\n",
    "print( recall)\n",
    "print('=' * 50)\n",
    "print(\"|Classification report|\")\n",
    "print('=' * 50)\n",
    "print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "#sử dụng Naive Bayes\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "clf= GaussianNB()\n",
    "clf.fit(X_train,y) #learning\n",
    "pred = clf.predict(X_test) #kết quả sau khi test\n",
    "recall = recall_score(y_test, pred,average='micro')\n",
    "precision = precision_score(y_test, pred,average='micro')\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "f1score= f1_score(y_test, pred, average='micro')\n",
    "print(\"Accuracy :\",score)\n",
    "print('=' * 50)\n",
    "print(\"F1 score :\",f1score)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, pred) #confusion matrix của nhẫn tấn công (test data) và kết quả sau khi test\n",
    "#sns.heatmap(cnf_matrix)\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "disp = plot_confusion_matrix(clf, X_test, y_test,ax=ax,cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "print('_' * 50)\n",
    "print(cnf_matrix)\n",
    "\n",
    "FPR= falseposrate(cnf_matrix, y_test, pred)\n",
    "print('=' * 50)\n",
    "print(\"|False positive Rate :|\")\n",
    "print('=' * 50)\n",
    "print(FPR)\n",
    "print('=' * 50)\n",
    "print(\"|Precision:|\")\n",
    "print('=' * 50)\n",
    "print(precision)\n",
    "print('=' * 50)\n",
    "print(\"|recall:|\")\n",
    "print('=' * 50)\n",
    "print( recall)\n",
    "print('=' * 50)\n",
    "print(\"|Classification report|\")\n",
    "print('=' * 50)\n",
    "print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phần này đang thử chưa hoàn thiện\n",
    "#Cải thiện hiệu suất bằng việc lựa chọn thuộc tính (feature selection)\n",
    "\n",
    "abs_corr = abs(corr)\n",
    "relevant_features = abs_corr[abs_corr>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= df_train[relevant_features.index]\n",
    "new_df_test= df_test[relevant_features.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cfs=new_df['attack_type']\n",
    "X_cfs=new_df.drop(['attack_type'],axis=1)\n",
    "\n",
    "y_test_cfs=new_df_test['attack_type']\n",
    "X_test_cfs=new_df_test.drop(['attack_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_cfs = sc.fit_transform(X_cfs)\n",
    "X_test_cfs = sc.transform(X_test_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def training(clf,xtrain,xtest,ytrain,ytest,attack_type):\n",
    "    print('\\n')\n",
    "    print('=' * 50)\n",
    "    print(\"Training \",attack_type)\n",
    "    print(clf)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    print('_' * 50)\n",
    "    pred = clf.predict(xtest)\n",
    "    print('_' * 50)\n",
    "    roc = roc_auc_score(ytest, clf.predict_proba(xtest), multi_class='ovo', average='weighted')\n",
    "    score = metrics.accuracy_score(ytest, pred)\n",
    "    f1score= f1_score(ytest, pred, average='micro')\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    print()\n",
    "    print('_' * 50)\n",
    "    print(\"|classification report|\")\n",
    "    print('_' * 50)\n",
    "    print(metrics.classification_report(ytest, pred))\n",
    "    print('_' * 50)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(ytest, pred))\n",
    "    cm= metrics.confusion_matrix(ytest, pred)\n",
    "    print()\n",
    "    print('_' * 50)\n",
    "    print(\"ROC AUC Score :\",roc)\n",
    "    FPR,precision,recall= falseposrate(cm,ytest,pred)\n",
    "    print('_' * 50)\n",
    "    print(\"False Positive Rate is :\",FPR)\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr,score, f1score,roc,FPR,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_CFS= []\n",
    "\n",
    "for clf, name in (\n",
    "        (GaussianNB() ,\"Naive Bayes\"),\n",
    "        # (KNeighborsClassifier(n_neighbors = 7),\"KNN\"),\n",
    "        #(OneVsRestClassifier(svm.SVC(probability=True)),\"One vs Rest SVM \"),\n",
    "        (RandomForestClassifier(), \"Random forest\"),(DecisionTreeClassifier(random_state=0),\"Decision Tree\"),\n",
    "        #(XGBClassifier(),\"XGBOOST\"),(svm.SVC(kernel='linear',probability=True),\"SVM Linear\"),\n",
    "        #(CatBoostClassifier(iterations=5,learning_rate=0.1),\"CAT Boost\")\n",
    "        ):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "\n",
    "    results_CFS.append(training(clf,X_train_cfs,X_test_cfs,y_cfs,y_test_cfs,\"CFS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train=pd.read_csv(\"NSL_KDD_Dataset/KDDTrain+.txt\")\n",
    "df_test=pd.read_csv(\"NSL_KDD_Dataset/KDDTest+.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True,axis=1) # For now, just drop NA's \n",
    "# (rows with missing values)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df_train.columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'attack_type',\n",
    "    'Score'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('Score',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "    \n",
    "# Encode text values to dummy variables(i.e. [1,0,0],\n",
    "# [0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_numeric_zscore(df_train, 'duration')\n",
    "encode_text_dummy(df_train, 'protocol_type')\n",
    "encode_text_dummy(df_train, 'service')\n",
    "encode_text_dummy(df_train, 'flag')\n",
    "encode_numeric_zscore(df_train, 'src_bytes')\n",
    "encode_numeric_zscore(df_train, 'dst_bytes')\n",
    "encode_text_dummy(df_train, 'land')\n",
    "encode_numeric_zscore(df_train, 'wrong_fragment')\n",
    "encode_numeric_zscore(df_train, 'urgent')\n",
    "encode_numeric_zscore(df_train, 'hot')\n",
    "encode_numeric_zscore(df_train, 'num_failed_logins')\n",
    "encode_text_dummy(df_train, 'logged_in')\n",
    "encode_numeric_zscore(df_train, 'num_compromised')\n",
    "encode_numeric_zscore(df_train, 'root_shell')\n",
    "encode_numeric_zscore(df_train, 'su_attempted')\n",
    "encode_numeric_zscore(df_train, 'num_root')\n",
    "encode_numeric_zscore(df_train, 'num_file_creations')\n",
    "encode_numeric_zscore(df_train, 'num_shells')\n",
    "encode_numeric_zscore(df_train, 'num_access_files')\n",
    "encode_numeric_zscore(df_train, 'num_outbound_cmds')\n",
    "encode_text_dummy(df_train, 'is_host_login')\n",
    "encode_text_dummy(df_train, 'is_guest_login')\n",
    "encode_numeric_zscore(df_train, 'count')\n",
    "encode_numeric_zscore(df_train, 'srv_count')\n",
    "encode_numeric_zscore(df_train, 'serror_rate')\n",
    "encode_numeric_zscore(df_train, 'srv_serror_rate')\n",
    "encode_numeric_zscore(df_train, 'rerror_rate')\n",
    "encode_numeric_zscore(df_train, 'srv_rerror_rate')\n",
    "encode_numeric_zscore(df_train, 'same_srv_rate')\n",
    "encode_numeric_zscore(df_train, 'diff_srv_rate')\n",
    "encode_numeric_zscore(df_train, 'srv_diff_host_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_count')\n",
    "encode_numeric_zscore(df_train, 'dst_host_srv_count')\n",
    "encode_numeric_zscore(df_train, 'dst_host_same_srv_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_diff_srv_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_same_src_port_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_srv_diff_host_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_serror_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_srv_serror_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_rerror_rate')\n",
    "encode_numeric_zscore(df_train, 'dst_host_srv_rerror_rate')\n",
    "\n",
    "df_train.dropna(inplace=True,axis=1)\n",
    "df_train[0:5]\n",
    "# This is the numeric feature vector, as it goes to the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phik\n",
    "from phik import resources, report\n",
    "corr_matrix=df_train.phik_matrix()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(corr_matrix[\"attack_type\"].sort_values(ascending=False)[1:])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "corr = corr_matrix[\"attack_type\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_sep={'normal':\"Normal\",'neptune':\"DOS\",\n",
    "            'satan':\"Probe\",'ipsweep':\"Probe\",'named':\"R2L\",\n",
    "            'ps':\"U2R\",'sendmail':\"R2L\",'xterm':\"U2R\",'xlock':\"R2L\",\n",
    "            'xsnoop':\"R2L\",'udpstorm':\"DOS\",'sqlattack':\"U2R\",'worm':\"DOS\",'portsweep':\"Probe\",\n",
    "            'smurf':\"DOS\",'nmap':\"Probe\",'back':\"DOS\",'mscan':\"Probe\",'apache2':\"DOS\",'processtable':\"DOS\",\n",
    "            'snmpguess':\"R2L\",'saint':\"Probe\",'mailbomb':\"DOS\",'snmpgetattack':\"R2L\",'httptunnel':\"R2L\",'teardrop':\"DOS\",\n",
    "            'warezclient':\"R2L\",'pod':\"DOS\",'guess_passwd':\"R2L\",'buffer_overflow':\"U2R\",'warezmaster':\"R2L\",'land':\"DOS\",'imap':\"R2L\",\n",
    "            'rootkit':\"U2R\",'loadmodule':\"U2R\",'ftp_write':\"R2L\",'multihop':\"R2L\",'phf':\"R2L\",'perl':\"U2R\",'spy':\"R2L\"}\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "df_train.replace({'attack_type':attack_sep},inplace=True)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Lets train a base model on the entire dataset and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy - Classification\n",
    "\n",
    "x_columns = df_train.columns.drop('attack_type')\n",
    "x = df_train[x_columns].values\n",
    "dummies = pd.get_dummies(df_train['attack_type']) # Classification\n",
    "outcomes = dummies.columns\n",
    "num_classes = len(outcomes)\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('attack_type')['attack_type'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2953/2953 - 4s - loss: 0.0262 - val_loss: 0.0455\n",
      "Epoch 44/1000\n",
      "2953/2953 - 5s - loss: 0.0269 - val_loss: 0.0353\n",
      "Epoch 45/1000\n",
      "2953/2953 - 5s - loss: 0.0260 - val_loss: 0.0376\n",
      "Epoch 46/1000\n",
      "2953/2953 - 5s - loss: 0.0273 - val_loss: 0.0386\n",
      "Epoch 47/1000\n",
      "2953/2953 - 8s - loss: 0.0261 - val_loss: 0.0394\n",
      "Epoch 48/1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "                        patience=5, verbose=1, mode='auto',\n",
    "                           restore_best_weights=True)\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "          callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "f1score= f1_score(y_eval, pred, average='micro')\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"f1 score: {}\".format(f1score))\n",
    "print(metrics.confusion_matrix(y_eval, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('=' * 50)\n",
    "print(\"Training \",\"CFS\")\n",
    "print(\"Deep learning\")\n",
    "print('_' * 50)\n",
    "pred\n",
    "print('_' * 50)\n",
    "roc = roc_auc_score(y_eval, model.predict_proba(x_test), multi_class='ovo', average='weighted')\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "f1score= f1_score(y_eval, pred, average='micro')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print()\n",
    "print('_' * 50)\n",
    "print(\"|classification report|\")\n",
    "print('_' * 50)\n",
    "print(metrics.classification_report(y_eval, pred))\n",
    "print('_' * 50)\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_eval, pred))\n",
    "cm= metrics.confusion_matrix(y_eval, pred)\n",
    "print()\n",
    "print('_' * 50)\n",
    "print(\"ROC AUC Score :\",roc)\n",
    "FPR,precision,recall= falseposrate(cm,y_eval,pred)\n",
    "print('_' * 50)\n",
    "print(\"False Positive Rate is :\",FPR)\n",
    "clf_descr = str(clf).split('(')[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65c7afee69072108e9b40987f1fefa28659afdb86f6b5c2042f19d65b6142350"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NCKH': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
